{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleConv2D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM88OB6IjHsWRDwSaau/9kn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaraaBold/DiverNaraa/blob/main/SimpleConv2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of 2D Convolutional Neural Network from scratch"
      ],
      "metadata": {
        "id": "zVg0VgHuLsPq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR1Q7xtDLZLy",
        "outputId": "5056341b-2bd3-444e-e771-ba08cb34e432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "uint8\n",
            "1.0\n",
            "0.0\n",
            "(60000,)\n",
            "(60000, 10)\n",
            "float64\n",
            "(48000, 28, 28)\n",
            "(12000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# data preperation\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape) # (60000, 28, 28)\n",
        "print(X_test.shape) # (10000, 28, 28)\n",
        "print(X_train[0].dtype) # uint8\n",
        "\n",
        "#Preprocessing \n",
        "X_train = X_train.astype(float)\n",
        "X_test = X_test.astype(float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "print(y_train.shape) # (60000,)\n",
        "print(y_train_one_hot.shape) # (60000, 10)\n",
        "print(y_train_one_hot.dtype) # float64\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "print(X_train.shape) # (48000, 28, 28)\n",
        "print(X_val.shape) # (12000, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "bmDIidmDTA7v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Creating a 2D convolutional layer"
      ],
      "metadata": {
        "id": "gZ_XmvYjMfjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConv2d():\n",
        "    def __init__(self, F, C, FH, FW, P, S,initializer=None,optimizer=None,activation=None):\n",
        "        self.P = P\n",
        "        self.S = S\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        self.W = self.initializer.W(F,C,FH,FW)\n",
        "        self.B = self.initializer.B(F)\n",
        "\n",
        "    def forward(self, X,debug=False):\n",
        "        '''\n",
        "        N - number of samples\n",
        "        C - number of channel\n",
        "        H - number of feature height\n",
        "        W - number of feature width\n",
        "        F - number of filters\n",
        "        FH - filter height\n",
        "        FW - filter width\n",
        "        OH - output height\n",
        "        OW - output width\n",
        "        '''\n",
        "        self.X = X\n",
        "        N,C,H,W = self.X.shape\n",
        "        F,C,FH,FW = self.W.shape\n",
        "        OH,OW = self.output_shape2d(H,W,self.P,self.P,FH,FW,self.S,self.S)\n",
        "        self.params = N,C,H,W,F,FH,FW,OH,OW\n",
        "        A = np.zeros([N,F,OH,OW])\n",
        "        self.X_pad = np.pad(self.X,((0,0),(0,0),(self.P,self.P),(self.P,self.P)))\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(0,H,self.S):\n",
        "                    for col in range(0,W,self.S):\n",
        "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
        "                            continue\n",
        "                        A[n,ch,row,col] = np.sum(self.X_pad[n,:,row:row+FH,col:col+FW]*self.W[ch,:,:,:]) +self.B[ch]\n",
        "        if debug==True:\n",
        "            return A\n",
        "        else:\n",
        "            return  self.activation.forward(A)\n",
        "            \n",
        "    def backward(self, dZ,debug=False):\n",
        "        if debug==True:\n",
        "            dA = dZ\n",
        "        else:\n",
        "            dA = self.activation.backward(dZ)\n",
        "        N,C,H,W,F,FH,FW,OH,OW = self.params\n",
        "        dZ = np.zeros(self.X_pad.shape)\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.dB = np.zeros(self.B.shape)\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(0,H,self.S):\n",
        "                    for col in range(0,W,self.S):\n",
        "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
        "                            continue\n",
        "                        dZ[n,:,row:row+FH,col:col+FW] += dA[n,ch,row,col]*self.W[ch,:,:,:]\n",
        "        if self.P == 0:\n",
        "            dZ = np.delete(dZ,[0,H-1],axis=2)\n",
        "            dZ = np.delete(dZ,[0,W-1],axis=3)\n",
        "        else:\n",
        "            dl_rows = range(self.P),range(H+self.P,H+2*self.P,1)\n",
        "            dl_cols = range(self.P),range(W+self.P,W+2*self.P,1)\n",
        "            dZ = np.delete(dZ,dl_rows,axis=2)\n",
        "            dZ = np.delete(dZ,dl_cols,axis=3)\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(OH):\n",
        "                    for col in range(OW):\n",
        "                        self.dW[ch,:,:,:] += dA[n,ch,row,col]*self.X_pad[n,:,row:row+FH,col:col+FW]\n",
        "        for ch in range(F):\n",
        "            self.dB[ch] = np.sum(dA[:,ch,:,:])\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ\n",
        "\n",
        "    def output_shape2d(self,H,W,PH,PW,FH,FW,SH,SW):\n",
        "        OH = (H +2*PH -FH)/SH +1\n",
        "        OW = (W +2*PW -FW)/SW +1\n",
        "        return int(OH),int(OW)"
      ],
      "metadata": {
        "id": "o5owOThuMh2n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializerConv2d:\n",
        "    def __init__(self, sigma=0.01):\n",
        "        self.sigma = sigma\n",
        "    def W(self, F, C, FH, FW):\n",
        "        return self.sigma * np.random.randn(F,C,FH,FW)\n",
        "    def B(self, F):\n",
        "        return np.zeros(F)"
      ],
      "metadata": {
        "id": "zltUDF5mMoVG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return"
      ],
      "metadata": {
        "id": "0ha_V2TWMqwO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 2] 2D convolutional layer experiment with small arrays"
      ],
      "metadata": {
        "id": "evMgItdpOHlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN2 のフォワードを流す時の入力データ\n",
        "# (1,1,4,4)\n",
        "x = np.array([[[[ 1,  2,  3,  4],\n",
        "                [ 5,  6,  7,  8],\n",
        "                [ 9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "\n",
        "# (2,3,3)\n",
        "w = np.array([[[[ 0.,  0.,  0.],\n",
        "               [ 0.,  1.,  0.],\n",
        "               [ 0., -1.,  0.]]],\n",
        "\n",
        "              [[[ 0.,  0.,  0.],\n",
        "               [ 0., -1.,  1.],\n",
        "               [ 0.,  0.,  0.]]]])\n",
        "\n",
        "simple_conv_2d = SimpleConv2d(F=2, C=1, FH=3, FW=3, P=0, S=1,initializer=SimpleInitializerConv2d(0.01),optimizer=SGD(0.01),activation=ReLU())\n",
        "simple_conv_2d.W = w\n",
        "A = simple_conv_2d.forward(x,True)\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw-r2ne5OHBW",
        "outputId": "a3474e05-cb6c-47fe-a031-20beb6116c65"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (?,1,2,2,)\n",
        "delta = np.array([[[[ -4,  -4],\n",
        "                   [ 10,  11]],\n",
        "\n",
        "                  [[  1,  -7],\n",
        "                   [  1, -11]]]])\n",
        "\n",
        "dZ = simple_conv_2d.backward(delta,True)\n",
        "print(dZ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uO975V7ONQe",
        "outputId": "e77af65f-65b3-4d0b-d862-2d180336c708"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-5.  4.]\n",
            "   [13. 27.]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 3] Output size after 2D convolution"
      ],
      "metadata": {
        "id": "q-u8tpquPvyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D():\n",
        "    def __init__(self,P):\n",
        "        self.P = P\n",
        "        self.PA = None\n",
        "        self.Pindex = None\n",
        "    def forward(self,A):\n",
        "        N,F,OH,OW = A.shape\n",
        "        PH,PW = int(OH/self.P),int(OW/self.P)\n",
        "        self.params = N,F,OH,OW,self.P,PH,PW\n",
        "        self.PA = np.zeros([N,F,PH,PW])\n",
        "        self.Pindex = np.zeros([N,F,PH,PW])\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        self.PA[n,ch,row,col] = np.max(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
        "                        self.Pindex[n,ch,row,col] = np.argmax(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
        "        return self.PA\n",
        "        \n",
        "    def backward(self,dA):\n",
        "        N,F,OH,OW,PS,PH,PW = self.params\n",
        "        dP = np.zeros([N,F,OH,OW])\n",
        "        for n in range(N): \n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        idx = self.Pindex[n,ch,row,col]\n",
        "                        tmp = np.zeros((PS*PS))\n",
        "                        for i in range(PS*PS):\n",
        "                            if i == idx:\n",
        "                                tmp[i] = dA[n,ch,row,col]\n",
        "                            else:\n",
        "                                tmp[i] = 0\n",
        "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
        "        return dP"
      ],
      "metadata": {
        "id": "h812woVuO3fn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "x = np.array([[[[ 1,  2,  3,  4],\n",
        "                [ 5,  6,  7,  8],\n",
        "                [ 9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "maxP = MaxPool2D(P=2)\n",
        "result = maxP.forward(x)\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPSLvsEwZw29",
        "outputId": "0098f8a5-15ac-4111-fc18-d79201e5605c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[ 6.  8.]\n",
            "   [14. 16.]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Exercise 5] (Advanced exercise) Creating an average pooling"
      ],
      "metadata": {
        "id": "sKz2FZcbQMrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgPool2D():\n",
        "    def __init__(self,P):\n",
        "        self.P = P\n",
        "        self.PA = None\n",
        "        self.Pindex = None\n",
        "    def forward(self,A):\n",
        "        N,F,OH,OW = A.shape\n",
        "        PH,PW = int(OH/self.P),int(OW/self.P)\n",
        "        self.params = N,F,OH,OW,self.P,PH,PW\n",
        "        self.PA = np.zeros([N,F,PH,PW])\n",
        "        self.Pindex = np.zeros([N,F,PH,PW])\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        self.PA[n,ch,row,col] = np.mean(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
        "                        self.Pindex[n,ch,row,col] = np.argmax(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
        "        return self.PA\n",
        "        \n",
        "    def backward(self,dA):\n",
        "        N,F,OH,OW,PS,PH,PW = self.params\n",
        "        dP = np.zeros([N,F,OH,OW])\n",
        "        for n in range(N): \n",
        "            for ch in range(F):\n",
        "                for row in range(PH):\n",
        "                    for col in range(PW):\n",
        "                        idx = self.Pindex[n,ch,row,col]\n",
        "                        tmp = np.zeros((PS*PS))\n",
        "                        for i in range(PS*PS):\n",
        "                            tmp[i] = dA[n,ch,row,col]/PS/PS\n",
        "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
        "        return dP"
      ],
      "metadata": {
        "id": "86p3qCGYQNRA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "x = np.array([[[[ 1,  2,  3,  4],\n",
        "                [ 5,  6,  7,  8],\n",
        "                [ 9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "avgP = AvgPool2D(P=2)\n",
        "result = avgP.forward(x)\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGvatZkhh3cM",
        "outputId": "5da7b24c-bb1f-48ed-a1f7-7b1379a09627"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[ 3.5  5.5]\n",
            "   [11.5 13.5]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 6] Smoothing"
      ],
      "metadata": {
        "id": "9njj4aiZRCK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self,X):\n",
        "        self.shape = X.shape\n",
        "        return X.reshape(len(X),-1)\n",
        "    def backward(self,X):\n",
        "        return X.reshape(self.shape)  "
      ],
      "metadata": {
        "id": "_pIzsfF-ScVn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 7] Learning and Estimation"
      ],
      "metadata": {
        "id": "laSMwZ3tSzaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a mini-batch generation iterator\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1] \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "Z3BpFLG9TITN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a Weight Initialization Class\n",
        "class XavierInitializer:\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "    \n",
        "class HeInitializer():\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "        \n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B"
      ],
      "metadata": {
        "id": "SzxiMcb1TsZl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Gradient Update Class\n",
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return\n",
        "\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 0\n",
        "        self.HB = 0\n",
        "    def update(self, layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr * np.sqrt(1/self.HW + 1e-7) * layer.dW\n",
        "        layer.B -= self.lr * np.sqrt(1/self.HB + 1e-7) * layer.dB"
      ],
      "metadata": {
        "id": "aktpghUSTtqN"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining FC layer class\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, Activater):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        self.Activer = Activater\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X@self.W + self.B\n",
        "        Z = self.Activer.forward(A)\n",
        "        return Z\n",
        "    def backward(self, dA):\n",
        "        dA = self.Activer.backward(dA)\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ"
      ],
      "metadata": {
        "id": "9S6wxYHET3kO"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definition of various activation functions\n",
        "class Sigmoid:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)"
      ],
      "metadata": {
        "id": "27N3lVkCT8IQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Scratch2dCNNClassifier():\n",
        "    def __init__(self, NN, CNN, n_epoch=5, n_batch=1, verbose = False):\n",
        "        self.NN = NN\n",
        "        self.CNN = CNN\n",
        "        self.n_epoch = n_epoch\n",
        "        self.n_batch = n_batch\n",
        "        self.verbose = verbose\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "        self.log_acc = np.zeros(self.n_epoch)\n",
        "\n",
        "    def loss_function(self,y,yt):\n",
        "        delta = 1e-7\n",
        "        return -np.mean(yt*np.log(y+delta))\n",
        "\n",
        "    def accuracy(self,Z,Y):\n",
        "        return accuracy_score(Y,Z)\n",
        "\n",
        "    def fit(self, X, y, X_val=False, y_val=False):\n",
        "        for epoch in range(self.n_epoch):\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
        "            self.loss = 0\n",
        "            for mini_X_train, mini_y_train in get_mini_batch:              \n",
        "                forward_data = mini_X_train[:,np.newaxis,:,:]\n",
        "                for layer in range(len(self.CNN)):\n",
        "                    forward_data = self.CNN[layer].forward(forward_data)\n",
        "                self.flt = Flatten()\n",
        "                forward_data = self.flt.forward(forward_data)\n",
        "                for layer in range(len(self.NN)):\n",
        "                    forward_data = self.NN[layer].forward(forward_data)\n",
        "                Z = forward_data\n",
        "                backward_data = (Z - mini_y_train)/self.n_batch\n",
        "                for layer in range(len(self.NN)-1,-1,-1):\n",
        "                    backward_data = self.NN[layer].backward(backward_data)\n",
        "                backward_data = self.flt.backward(backward_data)\n",
        "                for layer in range(len(self.CNN)-1,-1,-1):\n",
        "                    backward_data = self.CNN[layer].backward(backward_data)\n",
        "                self.loss += self.loss_function(Z,mini_y_train)\n",
        "                if self.verbose:\n",
        "                    print('batch loss %f'%self.loss_function(Z,mini_y_train))\n",
        "            if self.verbose:\n",
        "                print(self.loss/len(get_mini_batch),self.accuracy(self.predict(X),np.argmax(y,axis=1)))\n",
        "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
        "            self.log_acc[epoch] = self.accuracy(self.predict(X),np.argmax(y,axis=1))\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred_data = X[:,np.newaxis,:,:]\n",
        "        for layer in range(len(self.CNN)):\n",
        "            pred_data = self.CNN[layer].forward(pred_data)\n",
        "        pred_data = self.flt.forward(pred_data)\n",
        "        for layer in range(len(self.NN)):\n",
        "            pred_data = self.NN[layer].forward(pred_data)\n",
        "        return np.argmax(pred_data,axis=1)"
      ],
      "metadata": {
        "id": "4EDh_NT3Sz_2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN = {\n",
        "    0:FC(1960, 200, HeInitializer(), SGD(0.01), Tanh()),\n",
        "    1:FC(200, 200, HeInitializer(), SGD(0.01), Tanh()),\n",
        "    2:FC(200, 10, SimpleInitializer(0.01), SGD(0.01), Softmax()),\n",
        "}\n",
        "CNN = {\n",
        "    0:SimpleConv2d(F=10, C=1, FH=3, FW=3, P=1, S=1,initializer=SimpleInitializerConv2d(0.01),optimizer=SGD(0.01),activation=Tanh()),\n",
        "    1:MaxPool2D(2),\n",
        "}\n",
        "\n",
        "cnn1 = Scratch2dCNNClassifier(NN=NN,CNN=CNN,n_epoch=3,n_batch=64,verbose=True)\n",
        "cnn1.fit(X_train[:1000],y_train_one_hot[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcu6cNseTkje",
        "outputId": "12ba9fce-054a-4846-acfe-dad688a2821c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch loss 0.230074\n",
            "batch loss 0.230214\n",
            "batch loss 0.230352\n",
            "batch loss 0.230145\n",
            "batch loss 0.228094\n",
            "batch loss 0.301205\n",
            "batch loss 0.598638\n",
            "batch loss 0.835369\n",
            "batch loss 1.135721\n",
            "batch loss 1.188831\n",
            "batch loss 1.200005\n",
            "batch loss 1.027490\n",
            "batch loss 1.293009\n",
            "batch loss 1.162389\n",
            "batch loss 1.248332\n",
            "batch loss 1.397849\n",
            "0.783607310575274 0.092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch loss 1.336540\n",
            "batch loss 1.393196\n",
            "batch loss 1.322236\n",
            "batch loss 1.436786\n",
            "batch loss 1.585422\n",
            "batch loss 1.299498\n",
            "batch loss 1.380794\n",
            "batch loss 1.435503\n",
            "batch loss 1.418843\n",
            "batch loss 1.410273\n",
            "batch loss 1.295129\n",
            "batch loss 1.255269\n",
            "batch loss 1.183022\n",
            "batch loss 1.417240\n",
            "batch loss 1.293981\n",
            "batch loss 1.188104\n",
            "1.3532397505825289 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch loss 1.511048\n",
            "batch loss 1.051109\n",
            "batch loss 1.119941\n",
            "batch loss 1.225520\n",
            "batch loss 1.278548\n",
            "batch loss 1.322582\n",
            "batch loss 1.279718\n",
            "batch loss 1.356407\n",
            "batch loss 1.151414\n",
            "batch loss 1.536098\n",
            "batch loss 1.359657\n",
            "batch loss 1.058692\n",
            "batch loss 1.342189\n",
            "batch loss 1.281108\n",
            "batch loss 1.435517\n",
            "batch loss 1.085387\n",
            "1.2746833904480908 0.094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cnn1.predict(X_val[:500])\n",
        "accuracy = accuracy_score(np.argmax(y_val[:500],axis=1), y_pred)\n",
        "print('accuracy:{:.3f}'.format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3QhQQYgWsJV",
        "outputId": "5ee903b3-79eb-4488-8a98-a5b478137072"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:0.090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 8] (Advanced task) LeNet"
      ],
      "metadata": {
        "id": "p6BwKvQztdT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LeNet_NN = {\n",
        "    0:FC(784, 120, HeInitializer(), SGD(0.01), Sigmoid()),\n",
        "    1:FC(120, 84, HeInitializer(), SGD(0.01), Sigmoid()),\n",
        "    2:FC(84, 10, SimpleInitializer(0.01), SGD(0.01), Softmax()),\n",
        "}\n",
        "LeNet_CNN = {\n",
        "    0:SimpleConv2d(F=6, C=1, FH=5, FW=5, P=2, S=1,initializer=SimpleInitializerConv2d(0.01),optimizer=SGD(0.01),activation=Sigmoid()),\n",
        "    1:MaxPool2D(2),\n",
        "    2:SimpleConv2d(F=16, C=6, FH=5, FW=5, P=2, S=1,initializer=SimpleInitializerConv2d(0.01),optimizer=SGD(0.01),activation=Sigmoid()),\n",
        "    3:MaxPool2D(2),\n",
        "}\n",
        "\n",
        "cnn2 = Scratch2dCNNClassifier(NN=LeNet_NN,CNN=LeNet_CNN,n_epoch=3,n_batch=64,verbose=True)\n",
        "cnn2.fit(X_train[:1000],y_train_one_hot[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHRGb1cItfce",
        "outputId": "26b93b77-c55b-49d9-dd31-46765c36d25c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch loss 0.230808\n",
            "batch loss 0.230165\n",
            "batch loss 0.230313\n",
            "batch loss 0.230459\n",
            "batch loss 0.229922\n",
            "batch loss 0.230795\n",
            "batch loss 0.238916\n",
            "batch loss 0.250098\n",
            "batch loss 0.410887\n",
            "batch loss 0.461765\n",
            "batch loss 0.460870\n",
            "batch loss 0.430543\n",
            "batch loss 0.500187\n",
            "batch loss 0.598813\n",
            "batch loss 0.664161\n",
            "batch loss 0.786755\n",
            "0.3865909414448499 0.093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch loss 0.671285\n",
            "batch loss 0.723003\n",
            "batch loss 0.817410\n",
            "batch loss 0.702172\n",
            "batch loss 0.732150\n",
            "batch loss 0.842478\n",
            "batch loss 0.679857\n",
            "batch loss 0.839863\n",
            "batch loss 0.779890\n",
            "batch loss 0.722245\n",
            "batch loss 0.672916\n",
            "batch loss 0.879119\n",
            "batch loss 0.872802\n",
            "batch loss 1.022017\n",
            "batch loss 0.919089\n",
            "batch loss 1.047281\n",
            "0.8077234725358485 0.097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch loss 0.929558\n",
            "batch loss 0.946338\n",
            "batch loss 1.055599\n",
            "batch loss 1.022768\n",
            "batch loss 0.947137\n",
            "batch loss 1.072598\n",
            "batch loss 0.842164\n",
            "batch loss 0.998882\n",
            "batch loss 0.974568\n",
            "batch loss 1.091302\n",
            "batch loss 0.704805\n",
            "batch loss 0.928820\n",
            "batch loss 0.973565\n",
            "batch loss 0.807721\n",
            "batch loss 0.799134\n",
            "batch loss 0.600643\n",
            "0.9184751065479813 0.099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cnn2.predict(X_val[:500])\n",
        "accuracy = accuracy_score(np.argmax(y_val[:500],axis=1), y_pred)\n",
        "print('accuracy LeNet:{:.3f}'.format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8sJqWfviCr",
        "outputId": "468372b7-1ddd-4495-8067-7c4542739536"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy LeNet:0.076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 10] Calculation of output size and number of parameters"
      ],
      "metadata": {
        "id": "jvBgASwu346c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Output size = (input size + 2 x padding size - filter size) / stride size + 1 \n",
        "#number of parameters = filter size x input channel x output channel + output channel\n",
        "\n",
        "def calculate_size_paramaters (input_H, input_W, padding, filter_H, filter_W, stride, input_channel, output_channel):\n",
        "  #output_size = (input_H*input_W + 2 * padding - filter_H*filter_W) / (stride + 1)\n",
        "  output_height = (input_H + 2 * padding  - filter_H) / stride + 1\n",
        "  output_width = (input_W + 2 * padding  - filter_W) / stride + 1\n",
        "  parameters = filter_H * filter_W * input_channel * output_channel + output_channel\n",
        "  #print (output_height)\n",
        "  #print (output_width)\n",
        "  return output_height*output_width*output_channel, parameters\n",
        "\n",
        "in_H = 144 # input height\n",
        "in_W = 144 # input width\n",
        "pad = 0 # padding\n",
        "stride = 1 # stride\n",
        "in_channel = 3 #input channels\n",
        "filter_H = 3 # filter height\n",
        "filter_W = 3 # filter width\n",
        "out_channel = 6 # out channels\n",
        "print (\"Case 1: \" + str(calculate_size_paramaters(in_H, in_W, pad, filter_H, filter_W, stride, in_channel, out_channel)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYeRG3Si35jk",
        "outputId": "16d644e8-2126-44b8-896e-0c235e22ff14"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 1: (120984.0, 168)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_H = 60 # input height\n",
        "in_W = 60 # input width\n",
        "pad = 0 # padding\n",
        "stride = 1 # stride\n",
        "in_channel = 24 #input channels\n",
        "filter_H = 3 # filter height\n",
        "filter_W = 3 # filter width\n",
        "out_channel = 48 # out channels\n",
        "print (\"Case 2: \" + str(calculate_size_paramaters(in_H, in_W, pad, filter_H, filter_W, stride, in_channel, out_channel)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1_jsi9a-xfy",
        "outputId": "fab7fd45-e124-47e9-ac85-c2f3810f5205"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 2: (161472.0, 10416)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_H = 20 # input height\n",
        "in_W = 20 # input width\n",
        "pad = 0 # padding\n",
        "stride = 2 # stride\n",
        "in_channel = 10 #input channels\n",
        "filter_H = 3 # filter height\n",
        "filter_W = 3 # filter width\n",
        "out_channel = 20 # out channels\n",
        "print (\"Case 3: \" + str(calculate_size_paramaters(in_H, in_W, pad, filter_H, filter_W, stride, in_channel, out_channel)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_GwlHWw_Wip",
        "outputId": "d2b89ccc-df73-4c28-b88c-9506b9594d17"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 3: (1805.0, 1820)\n"
          ]
        }
      ]
    }
  ]
}