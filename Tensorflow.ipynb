{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDRn4lPvt+FKquYHBnEqDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaraaBold/DiverNaraa/blob/main/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 1] Looking back on the scratch"
      ],
      "metadata": {
        "id": "xUqFSLyE6LGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.test.gpu_device_name() \n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "# データセットの読み込み\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# データフレームから条件抽出\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# ラベルを数値に変換\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# load network structure                            \n",
        "logits = example_net(X)\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# estimation result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ursp3xOT_aQQ",
        "outputId": "979ffcd7-8006-44c7-c600-836b353e266b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.7085, val_loss : 6.4401, acc : 0.375\n",
            "Epoch 1, loss : 0.2747, val_loss : 2.7130, acc : 0.500\n",
            "Epoch 2, loss : 0.2105, val_loss : 2.4112, acc : 0.438\n",
            "Epoch 3, loss : 0.1319, val_loss : 1.6748, acc : 0.688\n",
            "Epoch 4, loss : 0.1312, val_loss : 1.6049, acc : 0.500\n",
            "Epoch 5, loss : 0.0863, val_loss : 1.4076, acc : 0.750\n",
            "Epoch 6, loss : 0.1074, val_loss : 1.2848, acc : 0.625\n",
            "Epoch 7, loss : 0.0688, val_loss : 1.2732, acc : 0.812\n",
            "Epoch 8, loss : 0.0883, val_loss : 1.1758, acc : 0.812\n",
            "Epoch 9, loss : 0.0628, val_loss : 1.1920, acc : 0.812\n",
            "Epoch 10, loss : 0.0700, val_loss : 1.1635, acc : 0.812\n",
            "Epoch 11, loss : 0.0607, val_loss : 1.1200, acc : 0.812\n",
            "Epoch 12, loss : 0.0607, val_loss : 1.1309, acc : 0.812\n",
            "Epoch 13, loss : 0.0586, val_loss : 1.0683, acc : 0.812\n",
            "Epoch 14, loss : 0.0562, val_loss : 1.0854, acc : 0.812\n",
            "Epoch 15, loss : 0.0560, val_loss : 1.0259, acc : 0.812\n",
            "Epoch 16, loss : 0.0532, val_loss : 1.0369, acc : 0.812\n",
            "Epoch 17, loss : 0.0532, val_loss : 0.9846, acc : 0.812\n",
            "Epoch 18, loss : 0.0506, val_loss : 0.9909, acc : 0.812\n",
            "Epoch 19, loss : 0.0506, val_loss : 0.9441, acc : 0.812\n",
            "Epoch 20, loss : 0.0484, val_loss : 0.9490, acc : 0.812\n",
            "Epoch 21, loss : 0.0485, val_loss : 0.9052, acc : 0.812\n",
            "Epoch 22, loss : 0.0465, val_loss : 0.9121, acc : 0.812\n",
            "Epoch 23, loss : 0.0467, val_loss : 0.8681, acc : 0.812\n",
            "Epoch 24, loss : 0.0448, val_loss : 0.8795, acc : 0.812\n",
            "Epoch 25, loss : 0.0452, val_loss : 0.8320, acc : 0.812\n",
            "Epoch 26, loss : 0.0432, val_loss : 0.8512, acc : 0.812\n",
            "Epoch 27, loss : 0.0440, val_loss : 0.7958, acc : 0.812\n",
            "Epoch 28, loss : 0.0417, val_loss : 0.8268, acc : 0.812\n",
            "Epoch 29, loss : 0.0429, val_loss : 0.7561, acc : 0.812\n",
            "Epoch 30, loss : 0.0400, val_loss : 0.8070, acc : 0.812\n",
            "Epoch 31, loss : 0.0420, val_loss : 0.7102, acc : 0.812\n",
            "Epoch 32, loss : 0.0381, val_loss : 0.7974, acc : 0.812\n",
            "Epoch 33, loss : 0.0416, val_loss : 0.6575, acc : 0.812\n",
            "Epoch 34, loss : 0.0360, val_loss : 0.8102, acc : 0.812\n",
            "Epoch 35, loss : 0.0422, val_loss : 0.6073, acc : 0.812\n",
            "Epoch 36, loss : 0.0339, val_loss : 0.8658, acc : 0.812\n",
            "Epoch 37, loss : 0.0446, val_loss : 0.5670, acc : 0.875\n",
            "Epoch 38, loss : 0.0328, val_loss : 0.9821, acc : 0.812\n",
            "Epoch 39, loss : 0.0498, val_loss : 0.5810, acc : 0.875\n",
            "Epoch 40, loss : 0.0353, val_loss : 1.1385, acc : 0.750\n",
            "Epoch 41, loss : 0.0567, val_loss : 0.6688, acc : 0.812\n",
            "Epoch 42, loss : 0.0417, val_loss : 1.1392, acc : 0.750\n",
            "Epoch 43, loss : 0.0578, val_loss : 0.6478, acc : 0.875\n",
            "Epoch 44, loss : 0.0428, val_loss : 1.0095, acc : 0.750\n",
            "Epoch 45, loss : 0.0521, val_loss : 0.5535, acc : 0.875\n",
            "Epoch 46, loss : 0.0366, val_loss : 0.9264, acc : 0.812\n",
            "Epoch 47, loss : 0.0473, val_loss : 0.5257, acc : 0.875\n",
            "Epoch 48, loss : 0.0329, val_loss : 0.8890, acc : 0.812\n",
            "Epoch 49, loss : 0.0450, val_loss : 0.5149, acc : 0.875\n",
            "Epoch 50, loss : 0.0314, val_loss : 0.8623, acc : 0.812\n",
            "Epoch 51, loss : 0.0436, val_loss : 0.5062, acc : 0.875\n",
            "Epoch 52, loss : 0.0303, val_loss : 0.8342, acc : 0.812\n",
            "Epoch 53, loss : 0.0423, val_loss : 0.4982, acc : 0.875\n",
            "Epoch 54, loss : 0.0292, val_loss : 0.8068, acc : 0.812\n",
            "Epoch 55, loss : 0.0410, val_loss : 0.4912, acc : 0.875\n",
            "Epoch 56, loss : 0.0282, val_loss : 0.7826, acc : 0.812\n",
            "Epoch 57, loss : 0.0398, val_loss : 0.4850, acc : 0.875\n",
            "Epoch 58, loss : 0.0273, val_loss : 0.7603, acc : 0.812\n",
            "Epoch 59, loss : 0.0386, val_loss : 0.4822, acc : 0.875\n",
            "Epoch 60, loss : 0.0264, val_loss : 0.7400, acc : 0.812\n",
            "Epoch 61, loss : 0.0372, val_loss : 0.4821, acc : 0.875\n",
            "Epoch 62, loss : 0.0255, val_loss : 0.7175, acc : 0.812\n",
            "Epoch 63, loss : 0.0358, val_loss : 0.4821, acc : 0.875\n",
            "Epoch 64, loss : 0.0247, val_loss : 0.6975, acc : 0.812\n",
            "Epoch 65, loss : 0.0347, val_loss : 0.4811, acc : 0.875\n",
            "Epoch 66, loss : 0.0240, val_loss : 0.6839, acc : 0.812\n",
            "Epoch 67, loss : 0.0339, val_loss : 0.4796, acc : 0.875\n",
            "Epoch 68, loss : 0.0235, val_loss : 0.6732, acc : 0.812\n",
            "Epoch 69, loss : 0.0332, val_loss : 0.4778, acc : 0.875\n",
            "Epoch 70, loss : 0.0230, val_loss : 0.6630, acc : 0.812\n",
            "Epoch 71, loss : 0.0326, val_loss : 0.4719, acc : 0.875\n",
            "Epoch 72, loss : 0.0226, val_loss : 0.6499, acc : 0.812\n",
            "Epoch 73, loss : 0.0320, val_loss : 0.4647, acc : 0.875\n",
            "Epoch 74, loss : 0.0222, val_loss : 0.6365, acc : 0.812\n",
            "Epoch 75, loss : 0.0315, val_loss : 0.4576, acc : 0.875\n",
            "Epoch 76, loss : 0.0218, val_loss : 0.6252, acc : 0.812\n",
            "Epoch 77, loss : 0.0310, val_loss : 0.4501, acc : 0.875\n",
            "Epoch 78, loss : 0.0213, val_loss : 0.6181, acc : 0.812\n",
            "Epoch 79, loss : 0.0308, val_loss : 0.4424, acc : 0.875\n",
            "Epoch 80, loss : 0.0211, val_loss : 0.6111, acc : 0.812\n",
            "Epoch 81, loss : 0.0306, val_loss : 0.4349, acc : 0.875\n",
            "Epoch 82, loss : 0.0208, val_loss : 0.6020, acc : 0.812\n",
            "Epoch 83, loss : 0.0302, val_loss : 0.4275, acc : 0.875\n",
            "Epoch 84, loss : 0.0206, val_loss : 0.5929, acc : 0.875\n",
            "Epoch 85, loss : 0.0299, val_loss : 0.4202, acc : 0.875\n",
            "Epoch 86, loss : 0.0202, val_loss : 0.5849, acc : 0.938\n",
            "Epoch 87, loss : 0.0296, val_loss : 0.4129, acc : 0.875\n",
            "Epoch 88, loss : 0.0199, val_loss : 0.5798, acc : 0.938\n",
            "Epoch 89, loss : 0.0294, val_loss : 0.4058, acc : 0.875\n",
            "Epoch 90, loss : 0.0198, val_loss : 0.5735, acc : 0.938\n",
            "Epoch 91, loss : 0.0292, val_loss : 0.3988, acc : 0.875\n",
            "Epoch 92, loss : 0.0196, val_loss : 0.5661, acc : 0.938\n",
            "Epoch 93, loss : 0.0290, val_loss : 0.3920, acc : 0.875\n",
            "Epoch 94, loss : 0.0194, val_loss : 0.5610, acc : 0.938\n",
            "Epoch 95, loss : 0.0288, val_loss : 0.3857, acc : 0.875\n",
            "Epoch 96, loss : 0.0193, val_loss : 0.5575, acc : 0.938\n",
            "Epoch 97, loss : 0.0288, val_loss : 0.3798, acc : 0.875\n",
            "Epoch 98, loss : 0.0193, val_loss : 0.5528, acc : 0.938\n",
            "Epoch 99, loss : 0.0287, val_loss : 0.3741, acc : 0.875\n",
            "test_acc : 0.850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The example_net() function defines a network structure the same as the Deep neural network task's network. But we implemented it faster using tensorflow's classes and functions.\n",
        "*   We defined loss function in loss_op using tf's sigmoid cross entropy function \n",
        "*   We defined Adam optimizer using tf's AdamOptimizer class. Also estimation result. \n",
        "*  Starts the training using tf's Session class. Loop in epochs, loop in mini-batches, and calculates the loss and accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p7SVqjeyAl1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 3] Create a model of Iris using all three types of objective variables"
      ],
      "metadata": {
        "id": "tTBbbCIrFFC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y[y == \"Iris-setosa\"] = 2\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FMGUFyfFGBo",
        "outputId": "adca6f41-e2bc-4fa9-adff-c603677c0707"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  del sys.path[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def iris_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# load network structure                            \n",
        "logits = iris_net(X)\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # softmax\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# estimation result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) #argmax\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnIodz01HYv3",
        "outputId": "8c3d5021-4dc2-4fb5-f0a0-0ea2759140ca"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1.9659, val_loss : 16.7750, acc : 0.208\n",
            "Epoch 1, loss : 1.4092, val_loss : 12.4761, acc : 0.167\n",
            "Epoch 2, loss : 0.9983, val_loss : 8.6795, acc : 0.292\n",
            "Epoch 3, loss : 0.7074, val_loss : 5.3429, acc : 0.250\n",
            "Epoch 4, loss : 0.4959, val_loss : 3.6381, acc : 0.292\n",
            "Epoch 5, loss : 0.3041, val_loss : 2.4643, acc : 0.458\n",
            "Epoch 6, loss : 0.1590, val_loss : 1.6986, acc : 0.583\n",
            "Epoch 7, loss : 0.1030, val_loss : 1.4265, acc : 0.708\n",
            "Epoch 8, loss : 0.0850, val_loss : 1.3812, acc : 0.708\n",
            "Epoch 9, loss : 0.0733, val_loss : 1.3221, acc : 0.708\n",
            "Epoch 10, loss : 0.0639, val_loss : 1.1580, acc : 0.792\n",
            "Epoch 11, loss : 0.0569, val_loss : 1.0631, acc : 0.792\n",
            "Epoch 12, loss : 0.0517, val_loss : 1.0115, acc : 0.750\n",
            "Epoch 13, loss : 0.0474, val_loss : 0.9467, acc : 0.750\n",
            "Epoch 14, loss : 0.0436, val_loss : 0.8994, acc : 0.792\n",
            "Epoch 15, loss : 0.0403, val_loss : 0.8647, acc : 0.792\n",
            "Epoch 16, loss : 0.0374, val_loss : 0.8248, acc : 0.792\n",
            "Epoch 17, loss : 0.0347, val_loss : 0.7883, acc : 0.833\n",
            "Epoch 18, loss : 0.0323, val_loss : 0.7562, acc : 0.833\n",
            "Epoch 19, loss : 0.0302, val_loss : 0.7313, acc : 0.833\n",
            "Epoch 20, loss : 0.0283, val_loss : 0.7161, acc : 0.792\n",
            "Epoch 21, loss : 0.0267, val_loss : 0.7023, acc : 0.792\n",
            "Epoch 22, loss : 0.0253, val_loss : 0.6816, acc : 0.792\n",
            "Epoch 23, loss : 0.0239, val_loss : 0.6591, acc : 0.792\n",
            "Epoch 24, loss : 0.0227, val_loss : 0.6395, acc : 0.792\n",
            "Epoch 25, loss : 0.0215, val_loss : 0.6227, acc : 0.792\n",
            "Epoch 26, loss : 0.0205, val_loss : 0.6070, acc : 0.792\n",
            "Epoch 27, loss : 0.0194, val_loss : 0.5899, acc : 0.792\n",
            "Epoch 28, loss : 0.0184, val_loss : 0.5739, acc : 0.792\n",
            "Epoch 29, loss : 0.0174, val_loss : 0.5571, acc : 0.792\n",
            "Epoch 30, loss : 0.0164, val_loss : 0.5389, acc : 0.792\n",
            "Epoch 31, loss : 0.0155, val_loss : 0.5237, acc : 0.792\n",
            "Epoch 32, loss : 0.0147, val_loss : 0.5100, acc : 0.792\n",
            "Epoch 33, loss : 0.0139, val_loss : 0.4924, acc : 0.792\n",
            "Epoch 34, loss : 0.0132, val_loss : 0.4764, acc : 0.792\n",
            "Epoch 35, loss : 0.0124, val_loss : 0.4614, acc : 0.792\n",
            "Epoch 36, loss : 0.0117, val_loss : 0.4472, acc : 0.792\n",
            "Epoch 37, loss : 0.0110, val_loss : 0.4341, acc : 0.792\n",
            "Epoch 38, loss : 0.0104, val_loss : 0.4222, acc : 0.792\n",
            "Epoch 39, loss : 0.0097, val_loss : 0.4109, acc : 0.833\n",
            "Epoch 40, loss : 0.0092, val_loss : 0.3996, acc : 0.833\n",
            "Epoch 41, loss : 0.0086, val_loss : 0.3874, acc : 0.833\n",
            "Epoch 42, loss : 0.0082, val_loss : 0.3780, acc : 0.833\n",
            "Epoch 43, loss : 0.0077, val_loss : 0.3691, acc : 0.833\n",
            "Epoch 44, loss : 0.0073, val_loss : 0.3611, acc : 0.833\n",
            "Epoch 45, loss : 0.0069, val_loss : 0.3542, acc : 0.833\n",
            "Epoch 46, loss : 0.0066, val_loss : 0.3478, acc : 0.833\n",
            "Epoch 47, loss : 0.0063, val_loss : 0.3437, acc : 0.833\n",
            "Epoch 48, loss : 0.0060, val_loss : 0.3399, acc : 0.833\n",
            "Epoch 49, loss : 0.0057, val_loss : 0.3359, acc : 0.833\n",
            "Epoch 50, loss : 0.0055, val_loss : 0.3331, acc : 0.833\n",
            "Epoch 51, loss : 0.0053, val_loss : 0.3308, acc : 0.833\n",
            "Epoch 52, loss : 0.0051, val_loss : 0.3292, acc : 0.833\n",
            "Epoch 53, loss : 0.0049, val_loss : 0.3276, acc : 0.833\n",
            "Epoch 54, loss : 0.0048, val_loss : 0.3246, acc : 0.833\n",
            "Epoch 55, loss : 0.0046, val_loss : 0.3231, acc : 0.833\n",
            "Epoch 56, loss : 0.0045, val_loss : 0.3232, acc : 0.833\n",
            "Epoch 57, loss : 0.0043, val_loss : 0.3234, acc : 0.875\n",
            "Epoch 58, loss : 0.0042, val_loss : 0.3242, acc : 0.875\n",
            "Epoch 59, loss : 0.0041, val_loss : 0.3231, acc : 0.875\n",
            "Epoch 60, loss : 0.0040, val_loss : 0.3211, acc : 0.875\n",
            "Epoch 61, loss : 0.0039, val_loss : 0.3212, acc : 0.875\n",
            "Epoch 62, loss : 0.0038, val_loss : 0.3203, acc : 0.875\n",
            "Epoch 63, loss : 0.0037, val_loss : 0.3191, acc : 0.875\n",
            "Epoch 64, loss : 0.0037, val_loss : 0.3184, acc : 0.875\n",
            "Epoch 65, loss : 0.0036, val_loss : 0.3189, acc : 0.875\n",
            "Epoch 66, loss : 0.0036, val_loss : 0.3179, acc : 0.875\n",
            "Epoch 67, loss : 0.0035, val_loss : 0.3177, acc : 0.875\n",
            "Epoch 68, loss : 0.0035, val_loss : 0.3187, acc : 0.875\n",
            "Epoch 69, loss : 0.0034, val_loss : 0.3180, acc : 0.875\n",
            "Epoch 70, loss : 0.0034, val_loss : 0.3190, acc : 0.875\n",
            "Epoch 71, loss : 0.0033, val_loss : 0.3200, acc : 0.875\n",
            "Epoch 72, loss : 0.0033, val_loss : 0.3194, acc : 0.875\n",
            "Epoch 73, loss : 0.0032, val_loss : 0.3198, acc : 0.875\n",
            "Epoch 74, loss : 0.0032, val_loss : 0.3206, acc : 0.875\n",
            "Epoch 75, loss : 0.0032, val_loss : 0.3209, acc : 0.875\n",
            "Epoch 76, loss : 0.0031, val_loss : 0.3210, acc : 0.875\n",
            "Epoch 77, loss : 0.0031, val_loss : 0.3207, acc : 0.875\n",
            "Epoch 78, loss : 0.0030, val_loss : 0.3206, acc : 0.875\n",
            "Epoch 79, loss : 0.0030, val_loss : 0.3207, acc : 0.875\n",
            "Epoch 80, loss : 0.0030, val_loss : 0.3210, acc : 0.875\n",
            "Epoch 81, loss : 0.0029, val_loss : 0.3218, acc : 0.875\n",
            "Epoch 82, loss : 0.0029, val_loss : 0.3216, acc : 0.875\n",
            "Epoch 83, loss : 0.0029, val_loss : 0.3216, acc : 0.875\n",
            "Epoch 84, loss : 0.0028, val_loss : 0.3214, acc : 0.875\n",
            "Epoch 85, loss : 0.0028, val_loss : 0.3227, acc : 0.875\n",
            "Epoch 86, loss : 0.0028, val_loss : 0.3240, acc : 0.875\n",
            "Epoch 87, loss : 0.0027, val_loss : 0.3233, acc : 0.875\n",
            "Epoch 88, loss : 0.0027, val_loss : 0.3252, acc : 0.875\n",
            "Epoch 89, loss : 0.0027, val_loss : 0.3266, acc : 0.875\n",
            "Epoch 90, loss : 0.0026, val_loss : 0.3271, acc : 0.875\n",
            "Epoch 91, loss : 0.0026, val_loss : 0.3284, acc : 0.875\n",
            "Epoch 92, loss : 0.0026, val_loss : 0.3291, acc : 0.875\n",
            "Epoch 93, loss : 0.0025, val_loss : 0.3312, acc : 0.875\n",
            "Epoch 94, loss : 0.0025, val_loss : 0.3313, acc : 0.875\n",
            "Epoch 95, loss : 0.0025, val_loss : 0.3323, acc : 0.875\n",
            "Epoch 96, loss : 0.0025, val_loss : 0.3336, acc : 0.875\n",
            "Epoch 97, loss : 0.0024, val_loss : 0.3342, acc : 0.875\n",
            "Epoch 98, loss : 0.0024, val_loss : 0.3349, acc : 0.875\n",
            "Epoch 99, loss : 0.0024, val_loss : 0.3351, acc : 0.833\n",
            "test_acc : 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 4] Create a House Prices model"
      ],
      "metadata": {
        "id": "1dOcjaiBIo43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation of House Price dataset\n",
        "dataset_path =\"train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbcDnuaKIpl6",
        "outputId": "05ff116d-e4e6-49f8-e0df-82937299131c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def house_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# load network structure                            \n",
        "logits = house_net(X)\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=logits) # mean squared error\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "MSE = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y, predictions=logits))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, mse = sess.run([loss_op, MSE], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, mse = sess.run([loss_op, MSE], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, mse : {:.3f}\".format(epoch, total_loss, val_loss, mse))\n",
        "    test_acc = sess.run(MSE, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_mse : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTVRPgrXJCJJ",
        "outputId": "9859ac30-d285-40e0-a01e-0a37db2fe272"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.3636, val_loss : 0.4428, mse : 0.443\n",
            "Epoch 1, loss : 0.0521, val_loss : 0.2568, mse : 0.257\n",
            "Epoch 2, loss : 0.0328, val_loss : 0.1869, mse : 0.187\n",
            "Epoch 3, loss : 0.0246, val_loss : 0.1244, mse : 0.124\n",
            "Epoch 4, loss : 0.0186, val_loss : 0.0969, mse : 0.097\n",
            "Epoch 5, loss : 0.0159, val_loss : 0.1142, mse : 0.114\n",
            "Epoch 6, loss : 0.0148, val_loss : 0.1681, mse : 0.168\n",
            "Epoch 7, loss : 0.0141, val_loss : 0.1896, mse : 0.190\n",
            "Epoch 8, loss : 0.0131, val_loss : 0.1561, mse : 0.156\n",
            "Epoch 9, loss : 0.0119, val_loss : 0.1241, mse : 0.124\n",
            "Epoch 10, loss : 0.0113, val_loss : 0.0919, mse : 0.092\n",
            "Epoch 11, loss : 0.0107, val_loss : 0.0699, mse : 0.070\n",
            "Epoch 12, loss : 0.0104, val_loss : 0.0624, mse : 0.062\n",
            "Epoch 13, loss : 0.0102, val_loss : 0.0618, mse : 0.062\n",
            "Epoch 14, loss : 0.0104, val_loss : 0.0603, mse : 0.060\n",
            "Epoch 15, loss : 0.0104, val_loss : 0.0581, mse : 0.058\n",
            "Epoch 16, loss : 0.0103, val_loss : 0.0567, mse : 0.057\n",
            "Epoch 17, loss : 0.0100, val_loss : 0.0588, mse : 0.059\n",
            "Epoch 18, loss : 0.0100, val_loss : 0.0614, mse : 0.061\n",
            "Epoch 19, loss : 0.0099, val_loss : 0.0674, mse : 0.067\n",
            "Epoch 20, loss : 0.0098, val_loss : 0.0744, mse : 0.074\n",
            "Epoch 21, loss : 0.0097, val_loss : 0.0804, mse : 0.080\n",
            "Epoch 22, loss : 0.0096, val_loss : 0.0879, mse : 0.088\n",
            "Epoch 23, loss : 0.0093, val_loss : 0.0882, mse : 0.088\n",
            "Epoch 24, loss : 0.0092, val_loss : 0.0929, mse : 0.093\n",
            "Epoch 25, loss : 0.0097, val_loss : 0.0959, mse : 0.096\n",
            "Epoch 26, loss : 0.0093, val_loss : 0.0954, mse : 0.095\n",
            "Epoch 27, loss : 0.0097, val_loss : 0.0963, mse : 0.096\n",
            "Epoch 28, loss : 0.0094, val_loss : 0.0915, mse : 0.091\n",
            "Epoch 29, loss : 0.0096, val_loss : 0.0898, mse : 0.090\n",
            "Epoch 30, loss : 0.0094, val_loss : 0.0831, mse : 0.083\n",
            "Epoch 31, loss : 0.0092, val_loss : 0.0810, mse : 0.081\n",
            "Epoch 32, loss : 0.0092, val_loss : 0.0788, mse : 0.079\n",
            "Epoch 33, loss : 0.0092, val_loss : 0.0773, mse : 0.077\n",
            "Epoch 34, loss : 0.0091, val_loss : 0.0748, mse : 0.075\n",
            "Epoch 35, loss : 0.0089, val_loss : 0.0712, mse : 0.071\n",
            "Epoch 36, loss : 0.0088, val_loss : 0.0682, mse : 0.068\n",
            "Epoch 37, loss : 0.0088, val_loss : 0.0660, mse : 0.066\n",
            "Epoch 38, loss : 0.0087, val_loss : 0.0633, mse : 0.063\n",
            "Epoch 39, loss : 0.0086, val_loss : 0.0636, mse : 0.064\n",
            "Epoch 40, loss : 0.0085, val_loss : 0.0626, mse : 0.063\n",
            "Epoch 41, loss : 0.0085, val_loss : 0.0602, mse : 0.060\n",
            "Epoch 42, loss : 0.0085, val_loss : 0.0570, mse : 0.057\n",
            "Epoch 43, loss : 0.0084, val_loss : 0.0550, mse : 0.055\n",
            "Epoch 44, loss : 0.0083, val_loss : 0.0559, mse : 0.056\n",
            "Epoch 45, loss : 0.0082, val_loss : 0.0548, mse : 0.055\n",
            "Epoch 46, loss : 0.0082, val_loss : 0.0545, mse : 0.054\n",
            "Epoch 47, loss : 0.0083, val_loss : 0.0562, mse : 0.056\n",
            "Epoch 48, loss : 0.0081, val_loss : 0.0536, mse : 0.054\n",
            "Epoch 49, loss : 0.0080, val_loss : 0.0544, mse : 0.054\n",
            "Epoch 50, loss : 0.0080, val_loss : 0.0536, mse : 0.054\n",
            "Epoch 51, loss : 0.0079, val_loss : 0.0544, mse : 0.054\n",
            "Epoch 52, loss : 0.0079, val_loss : 0.0536, mse : 0.054\n",
            "Epoch 53, loss : 0.0078, val_loss : 0.0554, mse : 0.055\n",
            "Epoch 54, loss : 0.0076, val_loss : 0.0561, mse : 0.056\n",
            "Epoch 55, loss : 0.0076, val_loss : 0.0546, mse : 0.055\n",
            "Epoch 56, loss : 0.0077, val_loss : 0.0550, mse : 0.055\n",
            "Epoch 57, loss : 0.0075, val_loss : 0.0525, mse : 0.052\n",
            "Epoch 58, loss : 0.0076, val_loss : 0.0546, mse : 0.055\n",
            "Epoch 59, loss : 0.0073, val_loss : 0.0530, mse : 0.053\n",
            "Epoch 60, loss : 0.0074, val_loss : 0.0538, mse : 0.054\n",
            "Epoch 61, loss : 0.0073, val_loss : 0.0529, mse : 0.053\n",
            "Epoch 62, loss : 0.0072, val_loss : 0.0533, mse : 0.053\n",
            "Epoch 63, loss : 0.0072, val_loss : 0.0527, mse : 0.053\n",
            "Epoch 64, loss : 0.0072, val_loss : 0.0521, mse : 0.052\n",
            "Epoch 65, loss : 0.0071, val_loss : 0.0525, mse : 0.053\n",
            "Epoch 66, loss : 0.0071, val_loss : 0.0519, mse : 0.052\n",
            "Epoch 67, loss : 0.0071, val_loss : 0.0518, mse : 0.052\n",
            "Epoch 68, loss : 0.0070, val_loss : 0.0514, mse : 0.051\n",
            "Epoch 69, loss : 0.0071, val_loss : 0.0515, mse : 0.051\n",
            "Epoch 70, loss : 0.0070, val_loss : 0.0511, mse : 0.051\n",
            "Epoch 71, loss : 0.0070, val_loss : 0.0514, mse : 0.051\n",
            "Epoch 72, loss : 0.0070, val_loss : 0.0508, mse : 0.051\n",
            "Epoch 73, loss : 0.0070, val_loss : 0.0507, mse : 0.051\n",
            "Epoch 74, loss : 0.0070, val_loss : 0.0507, mse : 0.051\n",
            "Epoch 75, loss : 0.0069, val_loss : 0.0505, mse : 0.051\n",
            "Epoch 76, loss : 0.0070, val_loss : 0.0505, mse : 0.050\n",
            "Epoch 77, loss : 0.0069, val_loss : 0.0505, mse : 0.051\n",
            "Epoch 78, loss : 0.0069, val_loss : 0.0502, mse : 0.050\n",
            "Epoch 79, loss : 0.0068, val_loss : 0.0500, mse : 0.050\n",
            "Epoch 80, loss : 0.0068, val_loss : 0.0501, mse : 0.050\n",
            "Epoch 81, loss : 0.0068, val_loss : 0.0501, mse : 0.050\n",
            "Epoch 82, loss : 0.0069, val_loss : 0.0497, mse : 0.050\n",
            "Epoch 83, loss : 0.0068, val_loss : 0.0494, mse : 0.049\n",
            "Epoch 84, loss : 0.0068, val_loss : 0.0497, mse : 0.050\n",
            "Epoch 85, loss : 0.0068, val_loss : 0.0495, mse : 0.049\n",
            "Epoch 86, loss : 0.0068, val_loss : 0.0490, mse : 0.049\n",
            "Epoch 87, loss : 0.0067, val_loss : 0.0495, mse : 0.049\n",
            "Epoch 88, loss : 0.0067, val_loss : 0.0492, mse : 0.049\n",
            "Epoch 89, loss : 0.0068, val_loss : 0.0494, mse : 0.049\n",
            "Epoch 90, loss : 0.0067, val_loss : 0.0497, mse : 0.050\n",
            "Epoch 91, loss : 0.0067, val_loss : 0.0497, mse : 0.050\n",
            "Epoch 92, loss : 0.0068, val_loss : 0.0493, mse : 0.049\n",
            "Epoch 93, loss : 0.0067, val_loss : 0.0495, mse : 0.049\n",
            "Epoch 94, loss : 0.0067, val_loss : 0.0492, mse : 0.049\n",
            "Epoch 95, loss : 0.0067, val_loss : 0.0492, mse : 0.049\n",
            "Epoch 96, loss : 0.0067, val_loss : 0.0489, mse : 0.049\n",
            "Epoch 97, loss : 0.0067, val_loss : 0.0499, mse : 0.050\n",
            "Epoch 98, loss : 0.0067, val_loss : 0.0490, mse : 0.049\n",
            "Epoch 99, loss : 0.0066, val_loss : 0.0489, mse : 0.049\n",
            "test_mse : 0.063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 5] Create an MNIST model"
      ],
      "metadata": {
        "id": "Lz55OMwYMfNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data preparation for MNIST \n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uL0nxCFMfp6",
        "outputId": "8cecb198-1120-4d9f-e96b-813a8367fd8d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 64 # changed the batch size and epoch\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "# load network structure                            \n",
        "logits = iris_net(X) # we use same network\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # softmax\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# estimation result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) #argmax\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCF9VP8mM6ef",
        "outputId": "55eafb00-a9c1-4762-d352-76198f570b9b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1.5802, val_loss : 32.1373, acc : 0.096\n",
            "Epoch 1, loss : 0.2359, val_loss : 6.9374, acc : 0.108\n",
            "Epoch 2, loss : 0.0728, val_loss : 3.3829, acc : 0.127\n",
            "Epoch 3, loss : 0.0464, val_loss : 2.7153, acc : 0.123\n",
            "Epoch 4, loss : 0.0404, val_loss : 2.5284, acc : 0.103\n",
            "Epoch 5, loss : 0.0385, val_loss : 2.4481, acc : 0.097\n",
            "Epoch 6, loss : 0.0375, val_loss : 2.4144, acc : 0.098\n",
            "Epoch 7, loss : 0.0370, val_loss : 2.3915, acc : 0.098\n",
            "Epoch 8, loss : 0.0367, val_loss : 2.3798, acc : 0.102\n",
            "Epoch 9, loss : 0.0365, val_loss : 2.3715, acc : 0.116\n",
            "test_acc : 0.115\n"
          ]
        }
      ]
    }
  ]
}